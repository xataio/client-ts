/**
 * Generated by @openapi-codegen
 *
 * @version 1.0
 */
import type * as Fetcher from './dataPlaneFetcher';
import { dataPlaneFetch, DataPlaneFetcherExtraProps } from './dataPlaneFetcher';
import type * as Schemas from './dataPlaneSchemas';
import type * as Responses from './dataPlaneResponses';

export type ListClusterBranchesPathParams = {
  /**
   * Cluster ID
   */
  clusterId: Schemas.ClusterID;
  workspace: string;
  region: string;
};

export type ListClusterBranchesQueryParams = {
  /**
   * Page size
   */
  page?: Schemas.PageSize;
  /**
   * Page token
   */
  token?: Schemas.PageToken;
};

export type ListClusterBranchesError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
>;

export type ListClusterBranchesVariables = {
  pathParams: ListClusterBranchesPathParams;
  queryParams?: ListClusterBranchesQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * Retrieve branches for given cluster ID
 */
export const listClusterBranches = (variables: ListClusterBranchesVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.ListClusterBranchesResponse,
    ListClusterBranchesError,
    undefined,
    {},
    ListClusterBranchesQueryParams,
    ListClusterBranchesPathParams
  >({
    url: '/cluster/{clusterId}/branches',
    method: 'get',
    ...variables,
    signal
  });

export type GetClusterMetricsPathParams = {
  /**
   * Cluster ID
   */
  clusterId: Schemas.ClusterID;
  workspace: string;
  region: string;
};

export type GetClusterMetricsQueryParams = {
  startTime?: string;
  endTime?: string;
  period?: '5min' | '15min' | '1hour';
  /**
   * Page size
   */
  page?: Schemas.PageSize;
  /**
   * Page token
   */
  token?: Schemas.PageToken;
};

export type GetClusterMetricsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetClusterMetricsVariables = {
  pathParams: GetClusterMetricsPathParams;
  queryParams?: GetClusterMetricsQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * retrieve a standard set of RDS cluster metrics
 */
export const getClusterMetrics = (variables: GetClusterMetricsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.MetricsResponse,
    GetClusterMetricsError,
    undefined,
    {},
    GetClusterMetricsQueryParams,
    GetClusterMetricsPathParams
  >({
    url: '/cluster/{clusterId}/metrics',
    method: 'get',
    ...variables,
    signal
  });

export type ApplyMigrationPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type ApplyMigrationError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type ApplyMigrationRequestBody = {
  /**
   * Migration name
   */
  name?: string;
  operations: {
    [key: string]: any;
  }[];
  adaptTables?: boolean;
};

export type ApplyMigrationVariables = {
  body: ApplyMigrationRequestBody;
  pathParams: ApplyMigrationPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Applies a pgroll migration to the specified database.
 */
export const applyMigration = (variables: ApplyMigrationVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.ApplyMigrationResponse,
    ApplyMigrationError,
    ApplyMigrationRequestBody,
    {},
    {},
    ApplyMigrationPathParams
  >({
    url: '/db/{dbBranchName}/migrations/apply',
    method: 'post',
    ...variables,
    signal
  });

export type StartMigrationPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type StartMigrationError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type StartMigrationRequestBody = {
  /**
   * Migration name
   */
  name?: string;
  operations: {
    [key: string]: any;
  }[];
  adaptTables?: boolean;
};

export type StartMigrationVariables = {
  body: StartMigrationRequestBody;
  pathParams: StartMigrationPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Starts a pgroll migration on the specified database.
 */
export const startMigration = (variables: StartMigrationVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.StartMigrationResponse,
    StartMigrationError,
    StartMigrationRequestBody,
    {},
    {},
    StartMigrationPathParams
  >({
    url: '/db/{dbBranchName}/migrations/start',
    method: 'post',
    ...variables,
    signal
  });

export type CompleteMigrationPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type CompleteMigrationError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type CompleteMigrationVariables = {
  pathParams: CompleteMigrationPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Complete an active migration on the specified database
 */
export const completeMigration = (variables: CompleteMigrationVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.CompleteMigrationResponse,
    CompleteMigrationError,
    undefined,
    {},
    {},
    CompleteMigrationPathParams
  >({
    url: '/db/{dbBranchName}/migrations/complete',
    method: 'post',
    ...variables,
    signal
  });

export type RollbackMigrationPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type RollbackMigrationError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type RollbackMigrationVariables = {
  pathParams: RollbackMigrationPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Roll back an active migration on the specified database
 */
export const rollbackMigration = (variables: RollbackMigrationVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.RollbackMigrationResponse,
    RollbackMigrationError,
    undefined,
    {},
    {},
    RollbackMigrationPathParams
  >({
    url: '/db/{dbBranchName}/migrations/rollback',
    method: 'post',
    ...variables,
    signal
  });

export type AdaptTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type AdaptTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type AdaptTableVariables = {
  pathParams: AdaptTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Adapt a table to be used from Xata, this will add the Xata metadata fields to the table, making it accessible through the data API.
 */
export const adaptTable = (variables: AdaptTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.ApplyMigrationResponse, AdaptTableError, undefined, {}, {}, AdaptTablePathParams>({
    url: '/db/{dbBranchName}/migrations/adapt/{tableName}',
    method: 'post',
    ...variables,
    signal
  });

export type AdaptAllTablesPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type AdaptAllTablesError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type AdaptAllTablesVariables = {
  pathParams: AdaptAllTablesPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Adapt all xata incompatible tables present in the branch, this will add the Xata metadata fields to the table, making them accessible through the data API.
 */
export const adaptAllTables = (variables: AdaptAllTablesVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.ApplyMigrationResponse, AdaptAllTablesError, undefined, {}, {}, AdaptAllTablesPathParams>({
    url: '/db/{dbBranchName}/migrations/adapt',
    method: 'post',
    ...variables,
    signal
  });

export type GetBranchMigrationJobStatusPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetBranchMigrationJobStatusError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetBranchMigrationJobStatusVariables = {
  pathParams: GetBranchMigrationJobStatusPathParams;
} & DataPlaneFetcherExtraProps;

export const getBranchMigrationJobStatus = (variables: GetBranchMigrationJobStatusVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.MigrationJobStatusResponse,
    GetBranchMigrationJobStatusError,
    undefined,
    {},
    {},
    GetBranchMigrationJobStatusPathParams
  >({
    url: '/db/{dbBranchName}/migrations/status',
    method: 'get',
    ...variables,
    signal
  });

export type GetMigrationJobsPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetMigrationJobsQueryParams = {
  /**
   * @format date-time
   */
  cursor?: string;
  /**
   * Page size
   */
  limit?: Schemas.PageSize;
};

export type GetMigrationJobsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetMigrationJobsVariables = {
  pathParams: GetMigrationJobsPathParams;
  queryParams?: GetMigrationJobsQueryParams;
} & DataPlaneFetcherExtraProps;

export const getMigrationJobs = (variables: GetMigrationJobsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.GetMigrationJobsResponse,
    GetMigrationJobsError,
    undefined,
    {},
    GetMigrationJobsQueryParams,
    GetMigrationJobsPathParams
  >({
    url: '/db/{dbBranchName}/migrations/jobs',
    method: 'get',
    ...variables,
    signal
  });

export type GetMigrationJobStatusPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The id of the migration job
   */
  jobId: Schemas.MigrationJobID;
  workspace: string;
  region: string;
};

export type GetMigrationJobStatusError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetMigrationJobStatusVariables = {
  pathParams: GetMigrationJobStatusPathParams;
} & DataPlaneFetcherExtraProps;

export const getMigrationJobStatus = (variables: GetMigrationJobStatusVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.MigrationJobStatusResponse,
    GetMigrationJobStatusError,
    undefined,
    {},
    {},
    GetMigrationJobStatusPathParams
  >({
    url: '/db/{dbBranchName}/migrations/jobs/{jobId}',
    method: 'get',
    ...variables,
    signal
  });

export type GetMigrationHistoryPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetMigrationHistoryQueryParams = {
  /**
   * @format date-time
   */
  cursor?: string;
  /**
   * Page size
   */
  limit?: Schemas.PageSize;
};

export type GetMigrationHistoryError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetMigrationHistoryVariables = {
  pathParams: GetMigrationHistoryPathParams;
  queryParams?: GetMigrationHistoryQueryParams;
} & DataPlaneFetcherExtraProps;

export const getMigrationHistory = (variables: GetMigrationHistoryVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.MigrationHistoryResponse,
    GetMigrationHistoryError,
    undefined,
    {},
    GetMigrationHistoryQueryParams,
    GetMigrationHistoryPathParams
  >({
    url: '/db/{dbBranchName}/migrations/history',
    method: 'get',
    ...variables,
    signal
  });

export type GetBranchListPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  workspace: string;
  region: string;
};

export type GetBranchListError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetBranchListVariables = {
  pathParams: GetBranchListPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * List all available Branches
 */
export const getBranchList = (variables: GetBranchListVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.ListBranchesResponse, GetBranchListError, undefined, {}, {}, GetBranchListPathParams>({
    url: '/dbs/{dbName}',
    method: 'get',
    ...variables,
    signal
  });

export type GetDatabaseSettingsPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  workspace: string;
  region: string;
};

export type GetDatabaseSettingsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.SimpleError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetDatabaseSettingsVariables = {
  pathParams: GetDatabaseSettingsPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Get database settings
 */
export const getDatabaseSettings = (variables: GetDatabaseSettingsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.DatabaseSettings, GetDatabaseSettingsError, undefined, {}, {}, GetDatabaseSettingsPathParams>({
    url: '/dbs/{dbName}/settings',
    method: 'get',
    ...variables,
    signal
  });

export type UpdateDatabaseSettingsPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  workspace: string;
  region: string;
};

export type UpdateDatabaseSettingsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.SimpleError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type UpdateDatabaseSettingsRequestBody = {
  searchEnabled?: boolean;
};

export type UpdateDatabaseSettingsVariables = {
  body?: UpdateDatabaseSettingsRequestBody;
  pathParams: UpdateDatabaseSettingsPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Update database settings, this endpoint can be used to disable search
 */
export const updateDatabaseSettings = (variables: UpdateDatabaseSettingsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.DatabaseSettings,
    UpdateDatabaseSettingsError,
    UpdateDatabaseSettingsRequestBody,
    {},
    {},
    UpdateDatabaseSettingsPathParams
  >({ url: '/dbs/{dbName}/settings', method: 'patch', ...variables, signal });

export type GetBranchDetailsPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetBranchDetailsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetBranchDetailsVariables = {
  pathParams: GetBranchDetailsPathParams;
} & DataPlaneFetcherExtraProps;

export const getBranchDetails = (variables: GetBranchDetailsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.DBBranch, GetBranchDetailsError, undefined, {}, {}, GetBranchDetailsPathParams>({
    url: '/db/{dbBranchName}',
    method: 'get',
    ...variables,
    signal
  });

export type CreateBranchPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type CreateBranchQueryParams = {
  /**
   * Name of source branch to branch the new schema from
   */
  from?: string;
};

export type CreateBranchError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 423;
      payload: Responses.SimpleError;
    }
>;

export type CreateBranchResponse = {
  /**
   * @minLength 1
   */
  databaseName: string;
  branchName: string;
  status: Schemas.MigrationStatus;
};

export type CreateBranchRequestBody = {
  /**
   * Select the branch to fork from. Defaults to 'main'
   */
  from?: string;
  /**
   * Select the dedicated cluster to create on. Defaults to 'xata-cloud'
   *
   * @minLength 1
   * @x-internal true
   */
  clusterID?: string;
  metadata?: Schemas.BranchMetadata;
};

export type CreateBranchVariables = {
  body?: CreateBranchRequestBody;
  pathParams: CreateBranchPathParams;
  queryParams?: CreateBranchQueryParams;
} & DataPlaneFetcherExtraProps;

export const createBranch = (variables: CreateBranchVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    CreateBranchResponse,
    CreateBranchError,
    CreateBranchRequestBody,
    {},
    CreateBranchQueryParams,
    CreateBranchPathParams
  >({ url: '/db/{dbBranchName}', method: 'put', ...variables, signal });

export type DeleteBranchPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type DeleteBranchError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 409;
      payload: Responses.SimpleError;
    }
>;

export type DeleteBranchResponse = {
  status: Schemas.MigrationStatus;
};

export type DeleteBranchVariables = {
  pathParams: DeleteBranchPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Delete the branch in the database and all its resources
 */
export const deleteBranch = (variables: DeleteBranchVariables, signal?: AbortSignal) =>
  dataPlaneFetch<DeleteBranchResponse, DeleteBranchError, undefined, {}, {}, DeleteBranchPathParams>({
    url: '/db/{dbBranchName}',
    method: 'delete',
    ...variables,
    signal
  });

export type GetSchemaPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetSchemaError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetSchemaResponse = {
  schema: Schemas.BranchSchema;
};

export type GetSchemaVariables = {
  pathParams: GetSchemaPathParams;
} & DataPlaneFetcherExtraProps;

export const getSchema = (variables: GetSchemaVariables, signal?: AbortSignal) =>
  dataPlaneFetch<GetSchemaResponse, GetSchemaError, undefined, {}, {}, GetSchemaPathParams>({
    url: '/db/{dbBranchName}/schema',
    method: 'get',
    ...variables,
    signal
  });

export type GetSchemasPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetSchemasError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetSchemasResponse = {
  schemas: Schemas.BranchSchema[];
};

export type GetSchemasVariables = {
  pathParams: GetSchemasPathParams;
} & DataPlaneFetcherExtraProps;

export const getSchemas = (variables: GetSchemasVariables, signal?: AbortSignal) =>
  dataPlaneFetch<GetSchemasResponse, GetSchemasError, undefined, {}, {}, GetSchemasPathParams>({
    url: '/db/{dbBranchName}/schemas',
    method: 'get',
    ...variables,
    signal
  });

export type CopyBranchPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type CopyBranchError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type CopyBranchRequestBody = {
  destinationBranch: string;
  limit?: number;
};

export type CopyBranchVariables = {
  body: CopyBranchRequestBody;
  pathParams: CopyBranchPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Create a copy of the branch
 */
export const copyBranch = (variables: CopyBranchVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.BranchWithCopyID, CopyBranchError, CopyBranchRequestBody, {}, {}, CopyBranchPathParams>({
    url: '/db/{dbBranchName}/copy',
    method: 'post',
    ...variables,
    signal
  });

export type GetBranchMoveStatusPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetBranchMoveStatusError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetBranchMoveStatusResponse = {
  state: string;
  pendingBytes: number;
};

export type GetBranchMoveStatusVariables = {
  pathParams: GetBranchMoveStatusPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Get the branch move status (if a move is happening)
 */
export const getBranchMoveStatus = (variables: GetBranchMoveStatusVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    GetBranchMoveStatusResponse,
    GetBranchMoveStatusError,
    undefined,
    {},
    {},
    GetBranchMoveStatusPathParams
  >({ url: '/db/{dbBranchName}/move', method: 'get', ...variables, signal });

export type MoveBranchPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type MoveBranchError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 423;
      payload: Responses.SimpleError;
    }
>;

export type MoveBranchResponse = {
  state: string;
};

export type MoveBranchRequestBody = {
  /**
   * Select the cluster to move the branch to. Must be different from the current cluster.
   *
   * @minLength 1
   * @x-internal true
   */
  to: string;
};

export type MoveBranchVariables = {
  body: MoveBranchRequestBody;
  pathParams: MoveBranchPathParams;
} & DataPlaneFetcherExtraProps;

export const moveBranch = (variables: MoveBranchVariables, signal?: AbortSignal) =>
  dataPlaneFetch<MoveBranchResponse, MoveBranchError, MoveBranchRequestBody, {}, {}, MoveBranchPathParams>({
    url: '/db/{dbBranchName}/move',
    method: 'put',
    ...variables,
    signal
  });

export type UpdateBranchMetadataPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type UpdateBranchMetadataError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type UpdateBranchMetadataVariables = {
  body?: Schemas.BranchMetadata;
  pathParams: UpdateBranchMetadataPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Update the branch metadata
 */
export const updateBranchMetadata = (variables: UpdateBranchMetadataVariables, signal?: AbortSignal) =>
  dataPlaneFetch<undefined, UpdateBranchMetadataError, Schemas.BranchMetadata, {}, {}, UpdateBranchMetadataPathParams>({
    url: '/db/{dbBranchName}/metadata',
    method: 'put',
    ...variables,
    signal
  });

export type GetBranchMetadataPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetBranchMetadataError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetBranchMetadataVariables = {
  pathParams: GetBranchMetadataPathParams;
} & DataPlaneFetcherExtraProps;

export const getBranchMetadata = (variables: GetBranchMetadataVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.BranchMetadata, GetBranchMetadataError, undefined, {}, {}, GetBranchMetadataPathParams>({
    url: '/db/{dbBranchName}/metadata',
    method: 'get',
    ...variables,
    signal
  });

export type GetBranchStatsPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetBranchStatsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.SimpleError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetBranchStatsResponse = {
  timestamp: string;
  interval: string;
  resolution: string;
  numberOfRecords?: Schemas.MetricsDatapoint[];
  writesOverTime?: Schemas.MetricsDatapoint[];
  readsOverTime?: Schemas.MetricsDatapoint[];
  readLatency?: Schemas.MetricsLatency;
  writeLatency?: Schemas.MetricsLatency;
  warning?: string;
};

export type GetBranchStatsVariables = {
  pathParams: GetBranchStatsPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Get branch usage metrics.
 */
export const getBranchStats = (variables: GetBranchStatsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<GetBranchStatsResponse, GetBranchStatsError, undefined, {}, {}, GetBranchStatsPathParams>({
    url: '/db/{dbBranchName}/stats',
    method: 'get',
    ...variables,
    signal
  });

export type GetGitBranchesMappingPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  workspace: string;
  region: string;
};

export type GetGitBranchesMappingError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
>;

export type GetGitBranchesMappingVariables = {
  pathParams: GetGitBranchesMappingPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Lists all the git branches in the mapping, and their associated Xata branches.
 *
 * Example response:
 *
 * ```json
 * {
 *   "mappings": [
 *       {
 *         "gitBranch": "main",
 *         "xataBranch": "main"
 *       },
 *       {
 *         "gitBranch": "gitBranch1",
 *         "xataBranch": "xataBranch1"
 *       }
 *       {
 *         "gitBranch": "xataBranch2",
 *         "xataBranch": "xataBranch2"
 *       }
 *   ]
 * }
 * ```
 */
export const getGitBranchesMapping = (variables: GetGitBranchesMappingVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.ListGitBranchesResponse,
    GetGitBranchesMappingError,
    undefined,
    {},
    {},
    GetGitBranchesMappingPathParams
  >({ url: '/dbs/{dbName}/gitBranches', method: 'get', ...variables, signal });

export type AddGitBranchesEntryPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  workspace: string;
  region: string;
};

export type AddGitBranchesEntryError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
>;

export type AddGitBranchesEntryResponse = {
  /**
   * Warning message
   */
  warning?: string;
};

export type AddGitBranchesEntryRequestBody = {
  /**
   * The name of the Git branch.
   */
  gitBranch: string;
  /**
   * The name of the Xata branch.
   */
  xataBranch: Schemas.BranchName;
};

export type AddGitBranchesEntryVariables = {
  body: AddGitBranchesEntryRequestBody;
  pathParams: AddGitBranchesEntryPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Adds an entry to the mapping of git branches to Xata branches. The git branch and the Xata branch must be present in the body of the request. If the Xata branch doesn't exist, a 400 error is returned.
 *
 * If the git branch is already present in the mapping, the old entry is overwritten, and a warning message is included in the response. If the git branch is added and didn't exist before, the response code is 204. If the git branch existed and it was overwritten, the response code is 201.
 *
 * Example request:
 *
 * ```json
 * // POST https://tutorial-ng7s8c.xata.sh/dbs/demo/gitBranches
 * {
 *   "gitBranch": "fix/bug123",
 *   "xataBranch": "fix_bug"
 * }
 * ```
 */
export const addGitBranchesEntry = (variables: AddGitBranchesEntryVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    AddGitBranchesEntryResponse,
    AddGitBranchesEntryError,
    AddGitBranchesEntryRequestBody,
    {},
    {},
    AddGitBranchesEntryPathParams
  >({ url: '/dbs/{dbName}/gitBranches', method: 'post', ...variables, signal });

export type RemoveGitBranchesEntryPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  workspace: string;
  region: string;
};

export type RemoveGitBranchesEntryQueryParams = {
  /**
   * The git branch to remove from the mapping
   */
  gitBranch: string;
};

export type RemoveGitBranchesEntryError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
>;

export type RemoveGitBranchesEntryVariables = {
  pathParams: RemoveGitBranchesEntryPathParams;
  queryParams: RemoveGitBranchesEntryQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * Removes an entry from the mapping of git branches to Xata branches. The name of the git branch must be passed as a query parameter. If the git branch is not found, the endpoint returns a 404 status code.
 *
 * Example request:
 *
 * ```json
 * // DELETE https://tutorial-ng7s8c.xata.sh/dbs/demo/gitBranches?gitBranch=fix%2Fbug123
 * ```
 */
export const removeGitBranchesEntry = (variables: RemoveGitBranchesEntryVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    undefined,
    RemoveGitBranchesEntryError,
    undefined,
    {},
    RemoveGitBranchesEntryQueryParams,
    RemoveGitBranchesEntryPathParams
  >({
    url: '/dbs/{dbName}/gitBranches',
    method: 'delete',
    ...variables,
    signal
  });

export type ResolveBranchPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  workspace: string;
  region: string;
};

export type ResolveBranchQueryParams = {
  /**
   * The Git Branch
   */
  gitBranch?: string;
  /**
   * Default branch to fallback to
   */
  fallbackBranch?: string;
};

export type ResolveBranchError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
>;

export type ResolveBranchResponse = {
  branch: string;
  reason: {
    code: 'FOUND_IN_MAPPING' | 'BRANCH_EXISTS' | 'FALLBACK_BRANCH' | 'DEFAULT_BRANCH';
    message: string;
  };
};

export type ResolveBranchVariables = {
  pathParams: ResolveBranchPathParams;
  queryParams?: ResolveBranchQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * In order to resolve the database branch, the following algorithm is used:
 * * if the `gitBranch` was provided and is found in the [git branches mapping](/docs/api-reference/dbs/db_name/gitBranches), the associated Xata branch is returned
 * * else, if a Xata branch with the exact same name as `gitBranch` exists, return it
 * * else, if `fallbackBranch` is provided and a branch with that name exists, return it
 * * else, return the default branch of the DB (`main` or the first branch)
 *
 * Example call:
 *
 * ```json
 * // GET https://tutorial-ng7s8c.xata.sh/dbs/demo/dbs/demo/resolveBranch?gitBranch=test&fallbackBranch=tsg
 * ```
 *
 * Example response:
 *
 * ```json
 * {
 *   "branch": "main",
 *   "reason": {
 *     "code": "DEFAULT_BRANCH",
 *     "message": "Default branch for this database (main)"
 *   }
 * }
 * ```
 */
export const resolveBranch = (variables: ResolveBranchVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    ResolveBranchResponse,
    ResolveBranchError,
    undefined,
    {},
    ResolveBranchQueryParams,
    ResolveBranchPathParams
  >({
    url: '/dbs/{dbName}/resolveBranch',
    method: 'get',
    ...variables,
    signal
  });

export type GetBranchMigrationHistoryPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetBranchMigrationHistoryError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetBranchMigrationHistoryResponse = {
  startedFrom?: Schemas.StartedFromMetadata;
  migrations?: Schemas.BranchMigration[];
};

export type GetBranchMigrationHistoryRequestBody = {
  limit?: number;
  startFrom?: string;
};

export type GetBranchMigrationHistoryVariables = {
  body?: GetBranchMigrationHistoryRequestBody;
  pathParams: GetBranchMigrationHistoryPathParams;
} & DataPlaneFetcherExtraProps;

export const getBranchMigrationHistory = (variables: GetBranchMigrationHistoryVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    GetBranchMigrationHistoryResponse,
    GetBranchMigrationHistoryError,
    GetBranchMigrationHistoryRequestBody,
    {},
    {},
    GetBranchMigrationHistoryPathParams
  >({
    url: '/db/{dbBranchName}/migrations',
    method: 'get',
    ...variables,
    signal
  });

export type GetBranchMigrationPlanPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetBranchMigrationPlanError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetBranchMigrationPlanVariables = {
  body: Schemas.Schema;
  pathParams: GetBranchMigrationPlanPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Compute a migration plan from a target schema the branch should be migrated too.
 */
export const getBranchMigrationPlan = (variables: GetBranchMigrationPlanVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.BranchMigrationPlan,
    GetBranchMigrationPlanError,
    Schemas.Schema,
    {},
    {},
    GetBranchMigrationPlanPathParams
  >({
    url: '/db/{dbBranchName}/migrations/plan',
    method: 'post',
    ...variables,
    signal
  });

export type ExecuteBranchMigrationPlanPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type ExecuteBranchMigrationPlanError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type ExecuteBranchMigrationPlanRequestBody = {
  version: number;
  migration: Schemas.BranchMigration;
};

export type ExecuteBranchMigrationPlanVariables = {
  body: ExecuteBranchMigrationPlanRequestBody;
  pathParams: ExecuteBranchMigrationPlanPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Apply a migration plan to the branch
 */
export const executeBranchMigrationPlan = (variables: ExecuteBranchMigrationPlanVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaUpdateResponse,
    ExecuteBranchMigrationPlanError,
    ExecuteBranchMigrationPlanRequestBody,
    {},
    {},
    ExecuteBranchMigrationPlanPathParams
  >({
    url: '/db/{dbBranchName}/migrations/execute',
    method: 'post',
    ...variables,
    signal
  });

export type QueryMigrationRequestsPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  workspace: string;
  region: string;
};

export type QueryMigrationRequestsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type QueryMigrationRequestsResponse = {
  migrationRequests: Schemas.MigrationRequest[];
  meta: Schemas.RecordsMetadata;
};

export type QueryMigrationRequestsRequestBody = {
  filter?: Schemas.FilterExpression;
  sort?: Schemas.SortExpression;
  page?: Schemas.PageConfig;
  columns?: Schemas.ColumnsProjection;
};

export type QueryMigrationRequestsVariables = {
  body?: QueryMigrationRequestsRequestBody;
  pathParams: QueryMigrationRequestsPathParams;
} & DataPlaneFetcherExtraProps;

export const queryMigrationRequests = (variables: QueryMigrationRequestsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    QueryMigrationRequestsResponse,
    QueryMigrationRequestsError,
    QueryMigrationRequestsRequestBody,
    {},
    {},
    QueryMigrationRequestsPathParams
  >({
    url: '/dbs/{dbName}/migrations/query',
    method: 'post',
    ...variables,
    signal
  });

export type CreateMigrationRequestPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  workspace: string;
  region: string;
};

export type CreateMigrationRequestError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type CreateMigrationRequestResponse = {
  number: number;
};

export type CreateMigrationRequestRequestBody = {
  /**
   * The source branch.
   */
  source: string;
  /**
   * The target branch.
   */
  target: string;
  /**
   * The title.
   */
  title: string;
  /**
   * Optional migration request description.
   */
  body?: string;
};

export type CreateMigrationRequestVariables = {
  body: CreateMigrationRequestRequestBody;
  pathParams: CreateMigrationRequestPathParams;
} & DataPlaneFetcherExtraProps;

export const createMigrationRequest = (variables: CreateMigrationRequestVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    CreateMigrationRequestResponse,
    CreateMigrationRequestError,
    CreateMigrationRequestRequestBody,
    {},
    {},
    CreateMigrationRequestPathParams
  >({ url: '/dbs/{dbName}/migrations', method: 'post', ...variables, signal });

export type GetMigrationRequestPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  /**
   * The migration request number.
   */
  mrNumber: Schemas.MigrationRequestNumber;
  workspace: string;
  region: string;
};

export type GetMigrationRequestError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetMigrationRequestVariables = {
  pathParams: GetMigrationRequestPathParams;
} & DataPlaneFetcherExtraProps;

export const getMigrationRequest = (variables: GetMigrationRequestVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.MigrationRequest, GetMigrationRequestError, undefined, {}, {}, GetMigrationRequestPathParams>({
    url: '/dbs/{dbName}/migrations/{mrNumber}',
    method: 'get',
    ...variables,
    signal
  });

export type UpdateMigrationRequestPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  /**
   * The migration request number.
   */
  mrNumber: Schemas.MigrationRequestNumber;
  workspace: string;
  region: string;
};

export type UpdateMigrationRequestError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type UpdateMigrationRequestRequestBody = {
  /**
   * New migration request title.
   */
  title?: string;
  /**
   * New migration request description.
   */
  body?: string;
  /**
   * Change the migration request status.
   */
  status?: 'open' | 'closed';
};

export type UpdateMigrationRequestVariables = {
  body?: UpdateMigrationRequestRequestBody;
  pathParams: UpdateMigrationRequestPathParams;
} & DataPlaneFetcherExtraProps;

export const updateMigrationRequest = (variables: UpdateMigrationRequestVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    undefined,
    UpdateMigrationRequestError,
    UpdateMigrationRequestRequestBody,
    {},
    {},
    UpdateMigrationRequestPathParams
  >({
    url: '/dbs/{dbName}/migrations/{mrNumber}',
    method: 'patch',
    ...variables,
    signal
  });

export type ListMigrationRequestsCommitsPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  /**
   * The migration request number.
   */
  mrNumber: Schemas.MigrationRequestNumber;
  workspace: string;
  region: string;
};

export type ListMigrationRequestsCommitsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type ListMigrationRequestsCommitsResponse = {
  meta: {
    /**
     * last record id
     */
    cursor: string;
    /**
     * true if more records can be fetch
     */
    more: boolean;
  };
  logs: Schemas.Commit[];
};

export type ListMigrationRequestsCommitsRequestBody = {
  page?: {
    /**
     * Query the next page that follow the cursor.
     */
    after?: string;
    /**
     * Query the previous page before the cursor.
     */
    before?: string;
    /**
     * Set page size. If the size is missing it is read from the cursor. If no cursor is given xata will choose the default page size.
     *
     * @default 20
     */
    size?: number;
  };
};

export type ListMigrationRequestsCommitsVariables = {
  body?: ListMigrationRequestsCommitsRequestBody;
  pathParams: ListMigrationRequestsCommitsPathParams;
} & DataPlaneFetcherExtraProps;

export const listMigrationRequestsCommits = (variables: ListMigrationRequestsCommitsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    ListMigrationRequestsCommitsResponse,
    ListMigrationRequestsCommitsError,
    ListMigrationRequestsCommitsRequestBody,
    {},
    {},
    ListMigrationRequestsCommitsPathParams
  >({
    url: '/dbs/{dbName}/migrations/{mrNumber}/commits',
    method: 'post',
    ...variables,
    signal
  });

export type CompareMigrationRequestPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  /**
   * The migration request number.
   */
  mrNumber: Schemas.MigrationRequestNumber;
  workspace: string;
  region: string;
};

export type CompareMigrationRequestError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type CompareMigrationRequestVariables = {
  pathParams: CompareMigrationRequestPathParams;
} & DataPlaneFetcherExtraProps;

export const compareMigrationRequest = (variables: CompareMigrationRequestVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaCompareResponse,
    CompareMigrationRequestError,
    undefined,
    {},
    {},
    CompareMigrationRequestPathParams
  >({
    url: '/dbs/{dbName}/migrations/{mrNumber}/compare',
    method: 'post',
    ...variables,
    signal
  });

export type GetMigrationRequestIsMergedPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  /**
   * The migration request number.
   */
  mrNumber: Schemas.MigrationRequestNumber;
  workspace: string;
  region: string;
};

export type GetMigrationRequestIsMergedError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetMigrationRequestIsMergedResponse = {
  merged?: boolean;
};

export type GetMigrationRequestIsMergedVariables = {
  pathParams: GetMigrationRequestIsMergedPathParams;
} & DataPlaneFetcherExtraProps;

export const getMigrationRequestIsMerged = (variables: GetMigrationRequestIsMergedVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    GetMigrationRequestIsMergedResponse,
    GetMigrationRequestIsMergedError,
    undefined,
    {},
    {},
    GetMigrationRequestIsMergedPathParams
  >({
    url: '/dbs/{dbName}/migrations/{mrNumber}/merge',
    method: 'get',
    ...variables,
    signal
  });

export type MergeMigrationRequestPathParams = {
  /**
   * The Database Name
   */
  dbName: Schemas.DBName;
  /**
   * The migration request number.
   */
  mrNumber: Schemas.MigrationRequestNumber;
  workspace: string;
  region: string;
};

export type MergeMigrationRequestError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type MergeMigrationRequestVariables = {
  pathParams: MergeMigrationRequestPathParams;
} & DataPlaneFetcherExtraProps;

export const mergeMigrationRequest = (variables: MergeMigrationRequestVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.BranchOp, MergeMigrationRequestError, undefined, {}, {}, MergeMigrationRequestPathParams>({
    url: '/dbs/{dbName}/migrations/{mrNumber}/merge',
    method: 'post',
    ...variables,
    signal
  });

export type GetBranchSchemaHistoryPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type GetBranchSchemaHistoryError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetBranchSchemaHistoryResponse = {
  meta: {
    /**
     * last record id
     */
    cursor: string;
    /**
     * true if more records can be fetch
     */
    more: boolean;
  };
  logs: Schemas.Commit[];
};

export type GetBranchSchemaHistoryRequestBody = {
  page?: {
    /**
     * Query the next page that follow the cursor.
     */
    after?: string;
    /**
     * Query the previous page before the cursor.
     */
    before?: string;
    /**
     * Set page size. If the size is missing it is read from the cursor. If no cursor is given xata will choose the default page size.
     *
     * @default 20
     */
    size?: number;
  };
  /**
   * Report only migrations that have been added since the given Migration ID.
   */
  since?: string;
};

export type GetBranchSchemaHistoryVariables = {
  body?: GetBranchSchemaHistoryRequestBody;
  pathParams: GetBranchSchemaHistoryPathParams;
} & DataPlaneFetcherExtraProps;

export const getBranchSchemaHistory = (variables: GetBranchSchemaHistoryVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    GetBranchSchemaHistoryResponse,
    GetBranchSchemaHistoryError,
    GetBranchSchemaHistoryRequestBody,
    {},
    {},
    GetBranchSchemaHistoryPathParams
  >({
    url: '/db/{dbBranchName}/schema/history',
    method: 'post',
    ...variables,
    signal
  });

export type CompareBranchWithUserSchemaPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type CompareBranchWithUserSchemaError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type CompareBranchWithUserSchemaRequestBody = {
  schema: Schemas.Schema;
  schemaOperations?: Schemas.MigrationOp[];
  branchOperations?: Schemas.MigrationOp[];
};

export type CompareBranchWithUserSchemaVariables = {
  body: CompareBranchWithUserSchemaRequestBody;
  pathParams: CompareBranchWithUserSchemaPathParams;
} & DataPlaneFetcherExtraProps;

export const compareBranchWithUserSchema = (variables: CompareBranchWithUserSchemaVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaCompareResponse,
    CompareBranchWithUserSchemaError,
    CompareBranchWithUserSchemaRequestBody,
    {},
    {},
    CompareBranchWithUserSchemaPathParams
  >({
    url: '/db/{dbBranchName}/schema/compare',
    method: 'post',
    ...variables,
    signal
  });

export type CompareBranchSchemasPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Database Name
   */
  branchName: Schemas.BranchName;
  workspace: string;
  region: string;
};

export type CompareBranchSchemasError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type CompareBranchSchemasRequestBody = {
  sourceBranchOperations?: Schemas.MigrationOp[];
  targetBranchOperations?: Schemas.MigrationOp[];
};

export type CompareBranchSchemasVariables = {
  body: CompareBranchSchemasRequestBody;
  pathParams: CompareBranchSchemasPathParams;
} & DataPlaneFetcherExtraProps;

export const compareBranchSchemas = (variables: CompareBranchSchemasVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaCompareResponse,
    CompareBranchSchemasError,
    CompareBranchSchemasRequestBody,
    {},
    {},
    CompareBranchSchemasPathParams
  >({
    url: '/db/{dbBranchName}/schema/compare/{branchName}',
    method: 'post',
    ...variables,
    signal
  });

export type UpdateBranchSchemaPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type UpdateBranchSchemaError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type UpdateBranchSchemaVariables = {
  body: Schemas.Migration;
  pathParams: UpdateBranchSchemaPathParams;
} & DataPlaneFetcherExtraProps;

export const updateBranchSchema = (variables: UpdateBranchSchemaVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaUpdateResponse,
    UpdateBranchSchemaError,
    Schemas.Migration,
    {},
    {},
    UpdateBranchSchemaPathParams
  >({
    url: '/db/{dbBranchName}/schema/update',
    method: 'post',
    ...variables,
    signal
  });

export type PreviewBranchSchemaEditPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type PreviewBranchSchemaEditError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type PreviewBranchSchemaEditResponse = {
  original: Schemas.Schema;
  updated: Schemas.Schema;
};

export type PreviewBranchSchemaEditRequestBody = {
  edits?: Schemas.SchemaEditScript;
};

export type PreviewBranchSchemaEditVariables = {
  body?: PreviewBranchSchemaEditRequestBody;
  pathParams: PreviewBranchSchemaEditPathParams;
} & DataPlaneFetcherExtraProps;

export const previewBranchSchemaEdit = (variables: PreviewBranchSchemaEditVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    PreviewBranchSchemaEditResponse,
    PreviewBranchSchemaEditError,
    PreviewBranchSchemaEditRequestBody,
    {},
    {},
    PreviewBranchSchemaEditPathParams
  >({
    url: '/db/{dbBranchName}/schema/preview',
    method: 'post',
    ...variables,
    signal
  });

export type ApplyBranchSchemaEditPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type ApplyBranchSchemaEditError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type ApplyBranchSchemaEditRequestBody = {
  edits: Schemas.SchemaEditScript;
};

export type ApplyBranchSchemaEditVariables = {
  body: ApplyBranchSchemaEditRequestBody;
  pathParams: ApplyBranchSchemaEditPathParams;
} & DataPlaneFetcherExtraProps;

export const applyBranchSchemaEdit = (variables: ApplyBranchSchemaEditVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaUpdateResponse,
    ApplyBranchSchemaEditError,
    ApplyBranchSchemaEditRequestBody,
    {},
    {},
    ApplyBranchSchemaEditPathParams
  >({
    url: '/db/{dbBranchName}/schema/apply',
    method: 'post',
    ...variables,
    signal
  });

export type PushBranchMigrationsPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type PushBranchMigrationsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type PushBranchMigrationsRequestBody = {
  migrations: Schemas.MigrationObject[];
};

export type PushBranchMigrationsVariables = {
  body: PushBranchMigrationsRequestBody;
  pathParams: PushBranchMigrationsPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * The `schema/push` API accepts a list of migrations to be applied to the
 * current branch. A list of applicable migrations can be fetched using
 * the `schema/history` API from another branch or database.
 *
 * The most recent migration must be part of the list or referenced (via
 * `parentID`) by the first migration in the list of migrations to be pushed.
 *
 * Each migration in the list has an `id`, `parentID`, and `checksum`. The
 * checksum for migrations are generated and verified by xata. The
 * operation fails if any migration in the list has an invalid checksum.
 */
export const pushBranchMigrations = (variables: PushBranchMigrationsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaUpdateResponse,
    PushBranchMigrationsError,
    PushBranchMigrationsRequestBody,
    {},
    {},
    PushBranchMigrationsPathParams
  >({
    url: '/db/{dbBranchName}/schema/push',
    method: 'post',
    ...variables,
    signal
  });

export type CreateTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type CreateTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 422;
      payload: Responses.SimpleError;
    }
>;

export type CreateTableResponse = {
  branchName: string;
  /**
   * @minLength 1
   */
  tableName: string;
  status: Schemas.MigrationStatus;
};

export type CreateTableVariables = {
  pathParams: CreateTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Creates a new table with the given name. Returns 422 if a table with the same name already exists.
 */
export const createTable = (variables: CreateTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<CreateTableResponse, CreateTableError, undefined, {}, {}, CreateTablePathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}',
    method: 'put',
    ...variables,
    signal
  });

export type DeleteTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type DeleteTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
>;

export type DeleteTableResponse = {
  status: Schemas.MigrationStatus;
};

export type DeleteTableVariables = {
  pathParams: DeleteTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Deletes the table with the given name.
 */
export const deleteTable = (variables: DeleteTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<DeleteTableResponse, DeleteTableError, undefined, {}, {}, DeleteTablePathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}',
    method: 'delete',
    ...variables,
    signal
  });

export type UpdateTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type UpdateTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 422;
      payload: Responses.SimpleError;
    }
>;

export type UpdateTableRequestBody = {
  /**
   * @minLength 1
   */
  name: string;
};

export type UpdateTableVariables = {
  body: UpdateTableRequestBody;
  pathParams: UpdateTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Update table. Currently there is only one update operation supported: renaming the table by providing a new name.
 *
 * In the example below, we rename a table from users to people:
 *
 * ```json
 * // PATCH /db/test:main/tables/users
 *
 * {
 *   "name": "people"
 * }
 * ```
 */
export const updateTable = (variables: UpdateTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaUpdateResponse,
    UpdateTableError,
    UpdateTableRequestBody,
    {},
    {},
    UpdateTablePathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}',
    method: 'patch',
    ...variables,
    signal
  });

export type GetTableSchemaPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type GetTableSchemaError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetTableSchemaResponse = {
  columns: Schemas.Column[];
};

export type GetTableSchemaVariables = {
  pathParams: GetTableSchemaPathParams;
} & DataPlaneFetcherExtraProps;

export const getTableSchema = (variables: GetTableSchemaVariables, signal?: AbortSignal) =>
  dataPlaneFetch<GetTableSchemaResponse, GetTableSchemaError, undefined, {}, {}, GetTableSchemaPathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/schema',
    method: 'get',
    ...variables,
    signal
  });

export type SetTableSchemaPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type SetTableSchemaError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 409;
      payload: Responses.SimpleError;
    }
>;

export type SetTableSchemaRequestBody = {
  columns: Schemas.Column[];
};

export type SetTableSchemaVariables = {
  body: SetTableSchemaRequestBody;
  pathParams: SetTableSchemaPathParams;
} & DataPlaneFetcherExtraProps;

export const setTableSchema = (variables: SetTableSchemaVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaUpdateResponse,
    SetTableSchemaError,
    SetTableSchemaRequestBody,
    {},
    {},
    SetTableSchemaPathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/schema',
    method: 'put',
    ...variables,
    signal
  });

export type GetTableColumnsPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type GetTableColumnsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetTableColumnsResponse = {
  columns: Schemas.Column[];
};

export type GetTableColumnsVariables = {
  pathParams: GetTableColumnsPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Retrieves the list of table columns and their definition. This endpoint returns the column list with object columns being reported with their
 * full dot-separated path (flattened).
 */
export const getTableColumns = (variables: GetTableColumnsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<GetTableColumnsResponse, GetTableColumnsError, undefined, {}, {}, GetTableColumnsPathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/columns',
    method: 'get',
    ...variables,
    signal
  });

export type AddTableColumnPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type AddTableColumnError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type AddTableColumnVariables = {
  body: Schemas.Column;
  pathParams: AddTableColumnPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Adds a new column to the table. The body of the request should contain the column definition.
 */
export const addTableColumn = (variables: AddTableColumnVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.SchemaUpdateResponse, AddTableColumnError, Schemas.Column, {}, {}, AddTableColumnPathParams>(
    {
      url: '/db/{dbBranchName}/tables/{tableName}/columns',
      method: 'post',
      ...variables,
      signal
    }
  );

export type GetColumnPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Column name
   */
  columnName: Schemas.ColumnName;
  workspace: string;
  region: string;
};

export type GetColumnError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetColumnVariables = {
  pathParams: GetColumnPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Get the definition of a single column.
 */
export const getColumn = (variables: GetColumnVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Schemas.Column, GetColumnError, undefined, {}, {}, GetColumnPathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/columns/{columnName}',
    method: 'get',
    ...variables,
    signal
  });

export type UpdateColumnPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Column name
   */
  columnName: Schemas.ColumnName;
  workspace: string;
  region: string;
};

export type UpdateColumnError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type UpdateColumnRequestBody = {
  /**
   * @minLength 1
   */
  name: string;
};

export type UpdateColumnVariables = {
  body: UpdateColumnRequestBody;
  pathParams: UpdateColumnPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Update column with partial data. Can be used for renaming the column by providing a new "name" field.
 */
export const updateColumn = (variables: UpdateColumnVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SchemaUpdateResponse,
    UpdateColumnError,
    UpdateColumnRequestBody,
    {},
    {},
    UpdateColumnPathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/columns/{columnName}',
    method: 'patch',
    ...variables,
    signal
  });

export type DeleteColumnPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Column name
   */
  columnName: Schemas.ColumnName;
  workspace: string;
  region: string;
};

export type DeleteColumnError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type DeleteColumnVariables = {
  pathParams: DeleteColumnPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Deletes the specified column.
 */
export const deleteColumn = (variables: DeleteColumnVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.SchemaUpdateResponse, DeleteColumnError, undefined, {}, {}, DeleteColumnPathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/columns/{columnName}',
    method: 'delete',
    ...variables,
    signal
  });

export type BranchTransactionPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type BranchTransactionError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Schemas.TransactionFailure;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 429;
      payload: Responses.RateLimitError;
    }
>;

export type BranchTransactionRequestBody = {
  operations: Schemas.TransactionOperation[];
};

export type BranchTransactionVariables = {
  body: BranchTransactionRequestBody;
  pathParams: BranchTransactionPathParams;
} & DataPlaneFetcherExtraProps;

export const branchTransaction = (variables: BranchTransactionVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Schemas.TransactionSuccess,
    BranchTransactionError,
    BranchTransactionRequestBody,
    {},
    {},
    BranchTransactionPathParams
  >({
    url: '/db/{dbBranchName}/transaction',
    method: 'post',
    ...variables,
    signal
  });

export type InsertRecordPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type InsertRecordQueryParams = {
  /**
   * Column filters
   */
  columns?: Schemas.ColumnsProjection;
};

export type InsertRecordError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type InsertRecordVariables = {
  body?: Schemas.DataInputRecord;
  pathParams: InsertRecordPathParams;
  queryParams?: InsertRecordQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * Insert a new Record into the Table
 */
export const insertRecord = (variables: InsertRecordVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.RecordUpdateResponse,
    InsertRecordError,
    Schemas.DataInputRecord,
    {},
    InsertRecordQueryParams,
    InsertRecordPathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/data',
    method: 'post',
    ...variables,
    signal
  });

export type GetFileItemPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  /**
   * The Column name
   */
  columnName: Schemas.ColumnName;
  /**
   * The File Identifier
   */
  fileId: Schemas.FileItemID;
  workspace: string;
  region: string;
};

export type GetFileItemError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetFileItemVariables = {
  pathParams: GetFileItemPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Retrieves file content from an array by file ID
 */
export const getFileItem = (variables: GetFileItemVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Blob, GetFileItemError, undefined, {}, {}, GetFileItemPathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}/column/{columnName}/file/{fileId}',
    method: 'get',
    ...variables,
    signal
  });

export type PutFileItemPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  /**
   * The Column name
   */
  columnName: Schemas.ColumnName;
  /**
   * The File Identifier
   */
  fileId: Schemas.FileItemID;
  workspace: string;
  region: string;
};

export type PutFileItemError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 422;
      payload: Responses.SimpleError;
    }
>;

export type PutFileItemVariables = {
  body?: Blob;
  pathParams: PutFileItemPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Uploads the file content to an array given the file ID
 */
export const putFileItem = (variables: PutFileItemVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.PutFileResponse, PutFileItemError, Blob, {}, {}, PutFileItemPathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}/column/{columnName}/file/{fileId}',
    method: 'put',
    ...variables,
    signal
  });

export type DeleteFileItemPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  /**
   * The Column name
   */
  columnName: Schemas.ColumnName;
  /**
   * The File Identifier
   */
  fileId: Schemas.FileItemID;
  workspace: string;
  region: string;
};

export type DeleteFileItemError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type DeleteFileItemVariables = {
  pathParams: DeleteFileItemPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Deletes an item from an file array column given the file ID
 */
export const deleteFileItem = (variables: DeleteFileItemVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.PutFileResponse, DeleteFileItemError, undefined, {}, {}, DeleteFileItemPathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}/column/{columnName}/file/{fileId}',
    method: 'delete',
    ...variables,
    signal
  });

export type GetFilePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  /**
   * The Column name
   */
  columnName: Schemas.ColumnName;
  workspace: string;
  region: string;
};

export type GetFileError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetFileVariables = {
  pathParams: GetFilePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Retrieves the file content from a file column
 */
export const getFile = (variables: GetFileVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Blob, GetFileError, undefined, {}, {}, GetFilePathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}/column/{columnName}/file',
    method: 'get',
    ...variables,
    signal
  });

export type PutFilePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  /**
   * The Column name
   */
  columnName: Schemas.ColumnName;
  workspace: string;
  region: string;
};

export type PutFileError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 422;
      payload: Responses.SimpleError;
    }
>;

export type PutFileVariables = {
  body?: Blob;
  pathParams: PutFilePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Uploads the file content to the given file column
 */
export const putFile = (variables: PutFileVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.PutFileResponse, PutFileError, Blob, {}, {}, PutFilePathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}/column/{columnName}/file',
    method: 'put',
    ...variables,
    signal
  });

export type DeleteFilePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  /**
   * The Column name
   */
  columnName: Schemas.ColumnName;
  workspace: string;
  region: string;
};

export type DeleteFileError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type DeleteFileVariables = {
  pathParams: DeleteFilePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Deletes a file referred in a file column
 */
export const deleteFile = (variables: DeleteFileVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.PutFileResponse, DeleteFileError, undefined, {}, {}, DeleteFilePathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}/column/{columnName}/file',
    method: 'delete',
    ...variables,
    signal
  });

export type GetRecordPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  workspace: string;
  region: string;
};

export type GetRecordQueryParams = {
  /**
   * Column filters
   */
  columns?: Schemas.ColumnsProjection;
};

export type GetRecordError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type GetRecordVariables = {
  pathParams: GetRecordPathParams;
  queryParams?: GetRecordQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * Retrieve record by ID
 */
export const getRecord = (variables: GetRecordVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.RecordResponse, GetRecordError, undefined, {}, GetRecordQueryParams, GetRecordPathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}',
    method: 'get',
    ...variables,
    signal
  });

export type InsertRecordWithIDPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  workspace: string;
  region: string;
};

export type InsertRecordWithIDQueryParams = {
  /**
   * Column filters
   */
  columns?: Schemas.ColumnsProjection;
  createOnly?: boolean;
  ifVersion?: number;
};

export type InsertRecordWithIDError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 422;
      payload: Responses.SimpleError;
    }
>;

export type InsertRecordWithIDVariables = {
  body?: Schemas.DataInputRecord;
  pathParams: InsertRecordWithIDPathParams;
  queryParams?: InsertRecordWithIDQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * By default, IDs are auto-generated when data is inserted into Xata. Sending a request to this endpoint allows us to insert a record with a pre-existing ID, bypassing the default automatic ID generation.
 */
export const insertRecordWithID = (variables: InsertRecordWithIDVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.RecordUpdateResponse,
    InsertRecordWithIDError,
    Schemas.DataInputRecord,
    {},
    InsertRecordWithIDQueryParams,
    InsertRecordWithIDPathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}',
    method: 'put',
    ...variables,
    signal
  });

export type UpdateRecordWithIDPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  workspace: string;
  region: string;
};

export type UpdateRecordWithIDQueryParams = {
  /**
   * Column filters
   */
  columns?: Schemas.ColumnsProjection;
  ifVersion?: number;
};

export type UpdateRecordWithIDError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 422;
      payload: Responses.SimpleError;
    }
>;

export type UpdateRecordWithIDVariables = {
  body?: Schemas.DataInputRecord;
  pathParams: UpdateRecordWithIDPathParams;
  queryParams?: UpdateRecordWithIDQueryParams;
} & DataPlaneFetcherExtraProps;

export const updateRecordWithID = (variables: UpdateRecordWithIDVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.RecordUpdateResponse,
    UpdateRecordWithIDError,
    Schemas.DataInputRecord,
    {},
    UpdateRecordWithIDQueryParams,
    UpdateRecordWithIDPathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}',
    method: 'patch',
    ...variables,
    signal
  });

export type UpsertRecordWithIDPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  workspace: string;
  region: string;
};

export type UpsertRecordWithIDQueryParams = {
  /**
   * Column filters
   */
  columns?: Schemas.ColumnsProjection;
  ifVersion?: number;
};

export type UpsertRecordWithIDError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 422;
      payload: Responses.SimpleError;
    }
>;

export type UpsertRecordWithIDVariables = {
  body?: Schemas.DataInputRecord;
  pathParams: UpsertRecordWithIDPathParams;
  queryParams?: UpsertRecordWithIDQueryParams;
} & DataPlaneFetcherExtraProps;

export const upsertRecordWithID = (variables: UpsertRecordWithIDVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.RecordUpdateResponse,
    UpsertRecordWithIDError,
    Schemas.DataInputRecord,
    {},
    UpsertRecordWithIDQueryParams,
    UpsertRecordWithIDPathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}',
    method: 'post',
    ...variables,
    signal
  });

export type DeleteRecordPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * The Record name
   */
  recordId: Schemas.RecordID;
  workspace: string;
  region: string;
};

export type DeleteRecordQueryParams = {
  /**
   * Column filters
   */
  columns?: Schemas.ColumnsProjection;
};

export type DeleteRecordError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type DeleteRecordVariables = {
  pathParams: DeleteRecordPathParams;
  queryParams?: DeleteRecordQueryParams;
} & DataPlaneFetcherExtraProps;

export const deleteRecord = (variables: DeleteRecordVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.RecordResponse,
    DeleteRecordError,
    undefined,
    {},
    DeleteRecordQueryParams,
    DeleteRecordPathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/data/{recordId}',
    method: 'delete',
    ...variables,
    signal
  });

export type BulkInsertTableRecordsPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type BulkInsertTableRecordsQueryParams = {
  /**
   * Column filters
   */
  columns?: Schemas.ColumnsProjection;
};

export type BulkInsertTableRecordsError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BulkError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 422;
      payload: Responses.SimpleError;
    }
>;

export type BulkInsertTableRecordsRequestBody = {
  records: Schemas.DataInputRecord[];
};

export type BulkInsertTableRecordsVariables = {
  body: BulkInsertTableRecordsRequestBody;
  pathParams: BulkInsertTableRecordsPathParams;
  queryParams?: BulkInsertTableRecordsQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * Bulk insert records
 */
export const bulkInsertTableRecords = (variables: BulkInsertTableRecordsVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.BulkInsertResponse,
    BulkInsertTableRecordsError,
    BulkInsertTableRecordsRequestBody,
    {},
    BulkInsertTableRecordsQueryParams,
    BulkInsertTableRecordsPathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/bulk',
    method: 'post',
    ...variables,
    signal
  });

export type QueryTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type QueryTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 503;
      payload: Responses.ServiceUnavailableError;
    }
>;

export type QueryTableRequestBody = {
  filter?: Schemas.FilterExpression;
  sort?: Schemas.SortExpression;
  page?: Schemas.PageConfig;
  columns?: Schemas.QueryColumnsProjection;
  /**
   * The consistency level for this request.
   *
   * @default strong
   */
  consistency?: 'strong' | 'eventual';
};

export type QueryTableVariables = {
  body?: QueryTableRequestBody;
  pathParams: QueryTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * The Query Table API can be used to retrieve all records in a table.
 * The API support filtering, sorting, selecting a subset of columns, and pagination.
 *
 * The overall structure of the request looks like this:
 *
 * ```json
 * // POST /db/<dbname>:<branch>/tables/<table>/query
 * {
 *   "columns": [...],
 *   "filter": {
 *     "$all": [...],
 *     "$any": [...]
 *     ...
 *   },
 *   "sort": {
 *     "multiple": [...]
 *     ...
 *   },
 *   "page": {
 *     ...
 *   }
 * }
 * ```
 *
 * For usage, see also the [Xata SDK documentation](https://xata.io/docs/sdk/get).
 *
 * ### Column selection
 *
 * If the `columns` array is not specified, all columns are included. For link
 * fields, only the ID column of the linked records is included in the response.
 *
 * If the `columns` array is specified, only the selected and internal
 * columns `id` and `xata` are included. The `*` wildcard can be used to
 * select all columns.
 *
 * For objects and link fields, if the column name of the object is specified, we
 * include all of its sub-keys. If only some sub-keys are specified (via dotted
 * notation, e.g. `"settings.plan"` ), then only those sub-keys from the object
 * are included.
 *
 * By the way of example, assuming two tables like this:
 *
 * ```json {"truncate": true}
 * {
 *   "tables": [
 *     {
 *       "name": "teams",
 *       "columns": [
 *         {
 *           "name": "name",
 *           "type": "string"
 *         },
 *         {
 *           "name": "owner",
 *           "type": "link",
 *           "link": {
 *             "table": "users"
 *           }
 *         },
 *         {
 *           "name": "foundedDate",
 *           "type": "datetime"
 *         },
 *       ]
 *     },
 *     {
 *       "name": "users",
 *       "columns": [
 *         {
 *           "name": "email",
 *           "type": "email"
 *         },
 *         {
 *           "name": "full_name",
 *           "type": "string"
 *         },
 *         {
 *           "name": "address",
 *           "type": "object",
 *           "columns": [
 *             {
 *               "name": "street",
 *               "type": "string"
 *             },
 *             {
 *               "name": "number",
 *               "type": "int"
 *             },
 *             {
 *               "name": "zipcode",
 *               "type": "int"
 *             }
 *           ]
 *         },
 *         {
 *           "name": "team",
 *           "type": "link",
 *           "link": {
 *             "table": "teams"
 *           }
 *         }
 *       ]
 *     }
 *   ]
 * }
 * ```
 *
 * A query like this:
 *
 * ```json
 * POST /db/<dbname>:<branch>/tables/<table>/query
 * {
 *   "columns": [
 *     "name",
 *     "address.*"
 *   ]
 * }
 * ```
 *
 * returns objects like:
 *
 * ```json
 * {
 *   "name": "Kilian",
 *   "address": {
 *     "street": "New street",
 *     "number": 41,
 *     "zipcode": 10407
 *   }
 * }
 * ```
 *
 * while a query like this:
 *
 * ```json
 * POST /db/<dbname>:<branch>/tables/<table>/query
 * {
 *   "columns": [
 *     "name",
 *     "address.street"
 *   ]
 * }
 * ```
 *
 * returns objects like:
 *
 * ```json
 * {
 *   "id": "id1"
 *   "xata": {
 *     "version": 0
 *   }
 *   "name": "Kilian",
 *   "address": {
 *     "street": "New street"
 *   }
 * }
 * ```
 *
 * If you want to return all columns from the main table and selected columns from the linked table, you can do it like this:
 *
 * ```json
 * {
 *   "columns": ["*", "team.name"]
 * }
 * ```
 *
 * The `"*"` in the above means all columns, including columns of objects. This returns data like:
 *
 * ```json
 * {
 *   "id": "id1"
 *   "xata": {
 *     "version": 0
 *   }
 *   "name": "Kilian",
 *   "email": "kilian@gmail.com",
 *   "address": {
 *     "street": "New street",
 *     "number": 41,
 *     "zipcode": 10407
 *   },
 *   "team": {
 *     "id": "XX",
 *     "xata": {
 *       "version": 0
 *     },
 *     "name": "first team"
 *   }
 * }
 * ```
 *
 * If you want all columns of the linked table, you can do:
 *
 * ```json
 * {
 *   "columns": ["*", "team.*"]
 * }
 * ```
 *
 * This returns, for example:
 *
 * ```json
 * {
 *   "id": "id1"
 *   "xata": {
 *     "version": 0
 *   }
 *   "name": "Kilian",
 *   "email": "kilian@gmail.com",
 *   "address": {
 *     "street": "New street",
 *     "number": 41,
 *     "zipcode": 10407
 *   },
 *   "team": {
 *     "id": "XX",
 *     "xata": {
 *       "version": 0
 *     },
 *     "name": "first team",
 *     "code": "A1",
 *     "foundedDate": "2020-03-04T10:43:54.32Z"
 *   }
 * }
 * ```
 *
 * ### Filtering
 *
 * There are two types of operators:
 *
 * - Operators that work on a single column: `$is`, `$contains`, `$pattern`,
 *   `$includes`, `$gt`, etc.
 * - Control operators that combine multiple conditions: `$any`, `$all`, `$not` ,
 *   `$none`, etc.
 *
 * All operators start with an `$` to differentiate them from column names
 * (which are not allowed to start with a dollar sign).
 *
 * #### Exact matching and control operators
 *
 * Filter by one column:
 *
 * ```json
 * {
 *   "filter": {
 *     "<column_name>": "value"
 *   }
 * }
 * ```
 *
 * This is equivalent to using the `$is` operator:
 *
 * ```json
 * {
 *   "filter": {
 *     "<column_name>": {
 *       "$is": "value"
 *     }
 *   }
 * }
 * ```
 *
 * For example:
 *
 * ```json
 * {
 *   "filter": {
 *     "name": "r2"
 *   }
 * }
 * ```
 *
 * Or:
 *
 * ```json
 * {
 *   "filter": {
 *     "name": {
 *       "$is": "r2"
 *     }
 *   }
 * }
 * ```
 *
 * For objects, both dots and nested versions work:
 *
 * ```json
 * {
 *   "filter": {
 *     "settings.plan": "free"
 *   }
 * }
 * ```
 *
 * ```json
 * {
 *   "filter": {
 *     "settings": {
 *       "plan": "free"
 *     }
 *   }
 * }
 * ```
 *
 * If you want to OR together multiple values, you can use the `$any` operator with an array of values:
 *
 * ```json
 * {
 *   "filter": {
 *     "settings.plan": { "$any": ["free", "paid"] }
 *   }
 * }
 * ```
 *
 * If you specify multiple columns in the same filter, they are logically AND'ed together:
 *
 * ```json
 * {
 *   "filter": {
 *     "settings.dark": true,
 *     "settings.plan": "free"
 *   }
 * }
 * ```
 *
 * The above matches if both conditions are met.
 *
 * To be more explicit about it, you can use `$all` or `$any`:
 *
 * ```json
 * {
 *   "filter": {
 *     "$any": {
 *       "settings.dark": true,
 *       "settings.plan": "free"
 *     }
 *   }
 * }
 * ```
 *
 * The `$all` and `$any` operators can also receive an array of objects, which allows for repeating column names:
 *
 * ```json
 * {
 *   "filter": {
 *     "$any": [
 *       {
 *         "name": "r1"
 *       },
 *       {
 *         "name": "r2"
 *       }
 *     ]
 *   }
 * }
 * ```
 *
 * You can check for a value being not-null with `$exists`:
 *
 * ```json
 * {
 *   "filter": {
 *     "$exists": "settings"
 *   }
 * }
 * ```
 *
 * This can be combined with `$all` or `$any` :
 *
 * ```json
 * {
 *   "filter": {
 *     "$all": [
 *       {
 *         "$exists": "settings"
 *       },
 *       {
 *         "$exists": "name"
 *       }
 *     ]
 *   }
 * }
 * ```
 *
 * Or you can use the inverse operator `$notExists`:
 *
 * ```json
 * {
 *   "filter": {
 *     "$notExists": "settings"
 *   }
 * }
 * ```
 *
 * #### Partial match
 *
 * `$contains` is the simplest operator for partial matching. Note that `$contains` operator can
 * cause performance issues at scale, because indices cannot be used.
 *
 * ```json
 * {
 *   "filter": {
 *     "<column_name>": {
 *       "$contains": "value"
 *     }
 *   }
 * }
 * ```
 *
 * Wildcards are supported via the `$pattern` operator:
 *
 * ```json
 * {
 *   "filter": {
 *     "<column_name>": {
 *       "$pattern": "v*alu?"
 *     }
 *   }
 * }
 * ```
 *
 * The `$pattern` operator accepts two wildcard characters:
 * * `*` matches zero or more characters
 * * `?` matches exactly one character
 *
 * If you want to match a string that contains a wildcard character, you can escape them using a backslash (`\`). You can escape a backslash by usign another backslash.
 *
 * You can also use the `$endsWith` and `$startsWith` operators:
 *
 * ```json
 * {
 *   "filter": {
 *     "<column_name>": {
 *       "$endsWith": ".gz"
 *     },
 *     "<column_name>": {
 *       "$startsWith": "tmp-"
 *     }
 *   }
 * }
 * ```
 *
 * #### Numeric or datetime ranges
 *
 * ```json
 * {
 *   "filter": {
 *     "<column_name>": {
 *       "$ge": 0,
 *       "$lt": 100
 *     }
 *   }
 * }
 * ```
 * Date ranges support the same operators, with the date using the format defined in
 * [RFC 3339](https://www.rfc-editor.org/rfc/rfc3339):
 * ```json
 * {
 *   "filter": {
 *     "<column_name>": {
 *       "$gt": "2019-10-12T07:20:50.52Z",
 *       "$lt": "2021-10-12T07:20:50.52Z"
 *     }
 *   }
 * }
 * ```
 * The supported operators are `$gt`, `$lt`, `$ge`, `$le`.
 *
 * #### Negations
 *
 * A general `$not` operator can inverse any operation.
 *
 * ```json
 * {
 *   "filter": {
 *     "$not": {
 *       "<column_name1>": "value1",
 *       "<column_name2>": "value1"
 *     }
 *   }
 * }
 * ```
 *
 * Note: in the above the two condition are AND together, so this does (NOT ( ...
 * AND ...))
 *
 * Or more complex:
 *
 * ```json
 * {
 *   "filter": {
 *     "$not": {
 *       "$any": [
 *         {
 *           "<column_name1>": "value1"
 *         },
 *         {
 *           "$all": [
 *             {
 *               "<column_name2>": "value2"
 *             },
 *             {
 *               "<column_name3>": "value3"
 *             }
 *           ]
 *         }
 *       ]
 *     }
 *   }
 * }
 * ```
 *
 * The `$not: { $any: {}}` can be shorted using the `$none` operator:
 *
 * ```json
 * {
 *   "filter": {
 *     "$none": {
 *       "<column_name1>": "value1",
 *       "<column_name2>": "value1"
 *     }
 *   }
 * }
 * ```
 *
 * In addition, you can use operators like `$isNot` or `$notExists` to simplify expressions:
 *
 * ```json
 * {
 *   "filter": {
 *     "<column_name>": {
 *       "$isNot": "2019-10-12T07:20:50.52Z"
 *     }
 *   }
 * }
 * ```
 *
 * #### Working with arrays
 *
 * To test that an array contains a value, use `$includesAny`.
 *
 * ```json
 * {
 *   "filter": {
 *     "<array_name>": {
 *       "$includesAny": "value"
 *     }
 *   }
 * }
 * ```
 *
 * ##### `includesAny`
 *
 * The `$includesAny` operator accepts a custom predicate that will check if
 * any value in the array column matches the predicate. The `$includes` operator is a
 * synonym for the `$includesAny` operator.
 *
 * For example a complex predicate can include
 * the `$all` , `$contains` and `$endsWith` operators:
 *
 * ```json
 * {
 *   "filter": {
 *     "<array name>": {
 *       "$includes": {
 *         "$all": [
 *           { "$contains": "label" },
 *           { "$not": { "$endsWith": "-debug" } }
 *         ]
 *       }
 *     }
 *   }
 * }
 * ```
 *
 * ##### `includesNone`
 *
 * The `$includesNone` operator succeeds if no array item matches the
 * predicate.
 *
 * ```json
 * {
 *   "filter": {
 *     "settings.labels": {
 *       "$includesNone": [{ "$contains": "label" }]
 *     }
 *   }
 * }
 * ```
 * The above matches if none of the array values contain the string "label".
 *
 * ##### `includesAll`
 *
 * The `$includesAll` operator succeeds if all array items match the
 * predicate.
 *
 * Here is an example of using the `$includesAll` operator:
 *
 * ```json
 * {
 *   "filter": {
 *     "settings.labels": {
 *       "$includesAll": [{ "$contains": "label" }]
 *     }
 *   }
 * }
 * ```
 *
 * The above matches if all array values contain the string "label".
 *
 * ### Sorting
 *
 * Sorting by one element:
 *
 * ```json
 * POST /db/demo:main/tables/table/query
 * {
 *   "sort": {
 *     "index": "asc"
 *   }
 * }
 * ```
 *
 * or descendently:
 *
 * ```json
 * POST /db/demo:main/tables/table/query
 * {
 *   "sort": {
 *     "index": "desc"
 *   }
 * }
 * ```
 *
 * Sorting by multiple fields:
 *
 * ```json
 * POST /db/demo:main/tables/table/query
 * {
 *   "sort": [
 *     {
 *       "index": "desc"
 *     },
 *     {
 *       "createdAt": "desc"
 *     }
 *   ]
 * }
 * ```
 *
 * It is also possible to sort results randomly:
 *
 * ```json
 * POST /db/demo:main/tables/table/query
 * {
 *   "sort": {
 *     "*": "random"
 *   }
 * }
 * ```
 *
 * Note that a random sort does not apply to a specific column, hence the special column name `"*"`.
 *
 * A random sort can be combined with an ascending or descending sort on a specific column:
 *
 * ```json
 * POST /db/demo:main/tables/table/query
 * {
 *   "sort": [
 *     {
 *       "name": "desc"
 *     },
 *     {
 *       "*": "random"
 *     }
 *   ]
 * }
 * ```
 *
 * This will sort on the `name` column, breaking ties randomly.
 *
 * ### Pagination
 *
 * We offer cursor pagination and offset pagination. The cursor pagination method can be used for sequential scrolling with unrestricted depth. The offset pagination can be used to skip pages and is limited to 1000 records.
 *
 * Example of cursor pagination:
 *
 * ```json
 * POST /db/demo:main/tables/table/query
 * {
 *   "page": {
 *     "after":"fMoxCsIwFIDh3WP8c4amDai5hO5SJCRNfaVSeC9b6d1FD"
 *   }
 * }
 * ```
 *
 * In the above example, the value of the `page.after` parameter is the cursor returned by the previous query. A sample response is shown below:
 *
 * ```json
 * {
 *   "meta": {
 *     "page": {
 *       "cursor": "fMoxCsIwFIDh3WP8c4amDai5hO5SJCRNfaVSeC9b6d1FD",
 *       "more": true
 *     }
 *   },
 *   "records": [...]
 * }
 * ```
 *
 * The `page` object might contain the follow keys, in addition to `size` and `offset` that were introduced before:
 *
 * - `after`: Return the next page 'after' the current cursor
 * - `before`: Return the previous page 'before' the current cursor.
 * - `start`: Resets the given cursor position to the beginning of the query result set.
 * Will return the first N records from the query result, where N is the `page.size` parameter.
 * - `end`: Resets the give cursor position to the end for the query result set.
 * Returns the last N records from the query result, where N is the `page.size` parameter.
 *
 * The request will fail if an invalid cursor value is given to `page.before`,
 * `page.after`, `page.start` , or `page.end`. No other cursor setting can be
 * used if `page.start` or `page.end` is set in a query.
 *
 * If both `page.before` and `page.after` parameters are present we treat the
 * request as a range query. The range query will return all entries after
 * `page.after`, but before `page.before`, up to `page.size` or the maximum
 * page size. This query requires both cursors to use the same filters and sort
 * settings, plus we require `page.after < page.before`. The range query returns
 * a new cursor. If the range encompass multiple pages the next page in the range
 * can be queried by update `page.after` to the returned cursor while keeping the
 * `page.before` cursor from the first range query.
 *
 * The `filter` , `columns`, `sort` , and `page.size` configuration will be
 * encoded with the cursor. The pagination request will be invalid if
 * `filter` or `sort` is set. The columns returned and page size can be changed
 * anytime by passing the `columns` or `page.size` settings to the next query.
 *
 * In the following example of size + offset pagination we retrieve the third page of up to 100 results:
 *
 * ```json
 * POST /db/demo:main/tables/table/query
 * {
 *   "page": {
 *     "size": 100,
 *     "offset": 200
 *   }
 * }
 * ```
 *
 * The `page.size` parameter represents the maximum number of records returned by this query. It has a default value of 20 and a maximum value of 200.
 * The `page.offset` parameter represents the number of matching records to skip. It has a default value of 0 and a maximum value of 800.
 *
 * Cursor pagination also works in combination with offset pagination. For example, starting from a specific cursor position, using a page size of 200 and an offset of 800, you can skip up to 5 pages of 200 records forwards or backwards from the cursor's position:
 *
 * ```json
 * POST /db/demo:main/tables/table/query
 * {
 *   "page": {
 *     "size": 200,
 *     "offset": 800,
 *     "after": "fMoxCsIwFIDh3WP8c4amDai5hO5SJCRNfaVSeC9b6d1FD"
 *   }
 * }
 * ```
 *
 * **Special cursors:**
 *
 * - `page.after=end`: Result points past the last entry. The list of records
 *   returned is empty, but `page.meta.cursor` will include a cursor that can be
 *   used to "tail" the table from the end waiting for new data to be inserted.
 * - `page.before=end`: This cursor returns the last page.
 * - `page.start=$cursor`: Start at the beginning of the result set of the $cursor query. This is equivalent to querying the
 *   first page without a cursor but applying `filter` and `sort` . Yet the `page.start`
 *   cursor can be convenient at times as user code does not need to remember the
 *   filter, sort, columns or page size configuration. All these information are
 *   read from the cursor.
 * - `page.end=$cursor`: Move to the end of the result set of the $cursor query. This is equivalent to querying the
 *   last page with `page.before=end`, `filter`, and `sort` . Yet the
 *   `page.end` cursor can be more convenient at times as user code does not
 *   need to remember the filter, sort, columns or page size configuration. All
 *   these information are read from the cursor.
 *
 * When using special cursors like `page.after="end"` or `page.before="end"`, we
 * still allow `filter` and `sort` to be set.
 *
 * Example of getting the last page:
 *
 * ```json
 * POST /db/demo:main/tables/table/query
 * {
 *   "page": {
 *     "size": 10,
 *     "before": "end"
 *   }
 * }
 * ```
 */
export const queryTable = (variables: QueryTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.QueryResponse, QueryTableError, QueryTableRequestBody, {}, {}, QueryTablePathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/query',
    method: 'post',
    ...variables,
    signal
  });

export type SearchBranchPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type SearchBranchError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 503;
      payload: Responses.ServiceUnavailableError;
    }
>;

export type SearchBranchRequestBody = {
  /**
   * An array with the tables in which to search. By default, all tables are included. Optionally, filters can be included that apply to each table.
   */
  tables?: (
    | string
    | {
        /**
         * The name of the table.
         */
        table: string;
        filter?: Schemas.FilterExpression;
        target?: Schemas.TargetExpression;
        boosters?: Schemas.BoosterExpression[];
      }
  )[];
  /**
   * The query string.
   *
   * @minLength 1
   */
  query: string;
  fuzziness?: Schemas.FuzzinessExpression;
  prefix?: Schemas.PrefixExpression;
  highlight?: Schemas.HighlightExpression;
  page?: Schemas.SearchPageConfig;
};

export type SearchBranchVariables = {
  body: SearchBranchRequestBody;
  pathParams: SearchBranchPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Run a free text search operation across the database branch.
 */
export const searchBranch = (variables: SearchBranchVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.SearchResponse, SearchBranchError, SearchBranchRequestBody, {}, {}, SearchBranchPathParams>({
    url: '/db/{dbBranchName}/search',
    method: 'post',
    ...variables,
    signal
  });

export type SearchTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type SearchTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type SearchTableRequestBody = {
  /**
   * The query string.
   *
   * @minLength 1
   */
  query: string;
  fuzziness?: Schemas.FuzzinessExpression;
  target?: Schemas.TargetExpression;
  prefix?: Schemas.PrefixExpression;
  filter?: Schemas.FilterExpression;
  highlight?: Schemas.HighlightExpression;
  boosters?: Schemas.BoosterExpression[];
  page?: Schemas.SearchPageConfig;
};

export type SearchTableVariables = {
  body: SearchTableRequestBody;
  pathParams: SearchTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Run a free text search operation in a particular table.
 *
 * The endpoint accepts a `query` parameter that is used for the free text search and a set of structured filters (via the `filter` parameter) that are applied before the search. The `filter` parameter uses the same syntax as the [query endpoint](/docs/api-reference/db/db_branch_name/tables/table_name/query#filtering) with the following exceptions:
 * * filters `$contains`, `$startsWith`, `$endsWith` don't work on columns of type `text`
 * * filtering on columns of type `multiple` is currently unsupported
 */
export const searchTable = (variables: SearchTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.SearchResponse, SearchTableError, SearchTableRequestBody, {}, {}, SearchTablePathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/search',
    method: 'post',
    ...variables,
    signal
  });

export type VectorSearchTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type VectorSearchTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type VectorSearchTableRequestBody = {
  /**
   * The vector to search for similarities. Must have the same dimension as
   * the vector column used.
   */
  queryVector: number[];
  /**
   * The vector column in which to search. It must be of type `vector`.
   */
  column: string;
  /**
   * The function used to measure the distance between two points. Can be one of:
   * `cosineSimilarity`, `l1`, `l2`. The default is `cosineSimilarity`.
   *
   * @default cosineSimilarity
   */
  similarityFunction?: string;
  /**
   * Number of results to return.
   *
   * @default 10
   * @maximum 100
   * @minimum 1
   */
  size?: number;
  filter?: Schemas.FilterExpression;
};

export type VectorSearchTableVariables = {
  body: VectorSearchTableRequestBody;
  pathParams: VectorSearchTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * This endpoint can be used to perform vector-based similarity searches in a table.
 * It can be used for implementing semantic search and product recommendation. To use this
 * endpoint, you need a column of type vector. The input vector must have the same
 * dimension as the vector column.
 */
export const vectorSearchTable = (variables: VectorSearchTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SearchResponse,
    VectorSearchTableError,
    VectorSearchTableRequestBody,
    {},
    {},
    VectorSearchTablePathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/vectorSearch',
    method: 'post',
    ...variables,
    signal
  });

export type AskTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type AskTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 429;
      payload: Responses.RateLimitError;
    }
>;

export type AskTableResponse = {
  /**
   * The answer to the input question
   */
  answer: string;
  /**
   * The session ID for the chat session.
   */
  sessionId: string;
};

export type AskTableRequestBody = {
  /**
   * The question you'd like to ask.
   *
   * @minLength 3
   */
  question: string;
  /**
   * The type of search to use. If set to `keyword` (the default), the search can be configured by passing
   * a `search` object with the following fields. For more details about each, see the Search endpoint documentation.
   * All fields are optional.
   *   * fuzziness  - typo tolerance
   *   * target - columns to search into, and weights.
   *   * prefix - prefix search type.
   *   * filter - pre-filter before searching.
   *   * boosters - control relevancy.
   * If set to `vector`, a `vectorSearch` object must be passed, with the following parameters. For more details, see the Vector
   * Search endpoint documentation. The `column` and `contentColumn` parameters are required.
   *   * column - the vector column containing the embeddings.
   *   * contentColumn - the column that contains the text from which the embeddings where computed.
   *   * filter - pre-filter before searching.
   *
   * @default keyword
   */
  searchType?: 'keyword' | 'vector';
  search?: {
    fuzziness?: Schemas.FuzzinessExpression;
    target?: Schemas.TargetExpression;
    prefix?: Schemas.PrefixExpression;
    filter?: Schemas.FilterExpression;
    boosters?: Schemas.BoosterExpression[];
  };
  vectorSearch?: {
    /**
     * The column to use for vector search. It must be of type `vector`.
     */
    column: string;
    /**
     * The column containing the text for vector search. Must be of type `text`.
     */
    contentColumn: string;
    filter?: Schemas.FilterExpression;
  };
  rules?: string[];
};

export type AskTableVariables = {
  body: AskTableRequestBody;
  pathParams: AskTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Ask your table a question. If the `Accept` header is set to `text/event-stream`, Xata will stream the results back as SSE's.
 */
export const askTable = (variables: AskTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<AskTableResponse, AskTableError, AskTableRequestBody, {}, {}, AskTablePathParams>({
    url: '/db/{dbBranchName}/tables/{tableName}/ask',
    method: 'post',
    ...variables,
    signal
  });

export type AskTableSessionPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  /**
   * @maxLength 36
   * @minLength 36
   */
  sessionId: string;
  workspace: string;
  region: string;
};

export type AskTableSessionError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 429;
      payload: Responses.RateLimitError;
    }
  | {
      status: 503;
      payload: Responses.ServiceUnavailableError;
    }
>;

export type AskTableSessionResponse = {
  /**
   * The answer to the input question
   */
  answer: string;
};

export type AskTableSessionRequestBody = {
  /**
   * The question you'd like to ask.
   *
   * @minLength 3
   */
  message?: string;
};

export type AskTableSessionVariables = {
  body?: AskTableSessionRequestBody;
  pathParams: AskTableSessionPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Ask a follow-up question. If the `Accept` header is set to `text/event-stream`, Xata will stream the results back as SSE's.
 */
export const askTableSession = (variables: AskTableSessionVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    AskTableSessionResponse,
    AskTableSessionError,
    AskTableSessionRequestBody,
    {},
    {},
    AskTableSessionPathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/ask/{sessionId}',
    method: 'post',
    ...variables,
    signal
  });

export type SummarizeTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type SummarizeTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type SummarizeTableRequestBody = {
  filter?: Schemas.FilterExpression;
  columns?: Schemas.ColumnsProjection;
  summaries?: Schemas.SummaryExpressionList;
  sort?: Schemas.SortExpression;
  summariesFilter?: Schemas.FilterExpression;
  /**
   * The consistency level for this request.
   *
   * @default strong
   */
  consistency?: 'strong' | 'eventual';
  page?: {
    /**
     * The number of records returned by summarize. If the amount of data you have exceeds this, or you have
     * more complex reporting requirements, we recommend that you use the aggregate endpoint instead.
     *
     * @default 20
     * @maximum 1000
     * @minimum 1
     */
    size?: number;
  };
};

export type SummarizeTableVariables = {
  body?: SummarizeTableRequestBody;
  pathParams: SummarizeTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * This endpoint allows you to (optionally) define groups, and then to run
 * calculations on the values in each group. This is most helpful when
 * you'd like to understand the data you have in your database.
 *
 * A group is a combination of unique values. If you create a group for
 * `sold_by`, `product_name`, we will return one row for every combination
 * of `sold_by` and `product_name` you have in your database. When you
 * want to calculate statistics, you define these groups and ask Xata to
 * calculate data on each group.
 *
 * **Some questions you can ask of your data:**
 *
 * How many records do I have in this table?
 * - Set `columns: []` as we we want data from the entire table, so we ask
 * for no groups.
 * - Set `summaries: {"total": {"count": "*"}}` in order to see the count
 * of all records. We use `count: *` here we'd like to know the total
 * amount of rows; ignoring whether they are `null` or not.
 *
 * What are the top total sales for each product in July 2022 and sold
 * more than 10 units?
 * - Set `filter: {soldAt: {
 *   "$ge": "2022-07-01T00:00:00.000Z",
 *   "$lt": "2022-08-01T00:00:00.000Z"}
 * }`
 * in order to limit the result set to sales recorded in July 2022.
 * - Set `columns: [product_name]` as we'd like to run calculations on
 * each unique product name in our table. Setting `columns` like this will
 * produce one row per unique product name.
 * - Set `summaries: {"total_sales": {"count": "product_name"}}` as we'd
 * like to create a field called "total_sales" for each group. This field
 * will count all rows in each group with non-null product names.
 * - Set `sort: [{"total_sales": "desc"}]` in order to bring the rows with
 * the highest total_sales field to the top.
 * - Set `summariesFilter: {"total_sales": {"$ge": 10}}` to only send back data
 * with greater than or equal to 10 units.
 *
 * `columns`: tells Xata how to create each group. If you add `product_id`
 * we will create a new group for every unique `product_id`.
 *
 * `summaries`: tells Xata which calculations to run on each group. Xata
 * currently supports count, min, max, sum, average.
 *
 * `sort`: tells Xata in which order you'd like to see results. You may
 * sort by fields specified in `columns` as well as the summary names
 * defined in `summaries`.
 *
 * note: Sorting on summarized values can be slower on very large tables;
 * this will impact your rate limit significantly more than other queries.
 * Try use `filter` to reduce the amount of data being processed in order
 * to reduce impact on your limits.
 *
 * `summariesFilter`: tells Xata how to filter the results of a summary.
 * It has the same syntax as `filter`, however, by using `summariesFilter`
 * you may also filter on the results of a query.
 *
 * note: This is a much slower to use than `filter`. We recommend using
 * `filter` wherever possible and `summariesFilter` when it's not
 * possible to use `filter`.
 *
 * `page.size`: tells Xata how many records to return. If unspecified, Xata
 * will return the default size.
 */
export const summarizeTable = (variables: SummarizeTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SummarizeResponse,
    SummarizeTableError,
    SummarizeTableRequestBody,
    {},
    {},
    SummarizeTablePathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/summarize',
    method: 'post',
    ...variables,
    signal
  });

export type AggregateTablePathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  /**
   * The Table name
   */
  tableName: Schemas.TableName;
  workspace: string;
  region: string;
};

export type AggregateTableError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type AggregateTableRequestBody = {
  filter?: Schemas.FilterExpression;
  aggs?: Schemas.AggExpressionMap;
};

export type AggregateTableVariables = {
  body?: AggregateTableRequestBody;
  pathParams: AggregateTablePathParams;
} & DataPlaneFetcherExtraProps;

/**
 * This endpoint allows you to run aggregations (analytics) on the data from one table.
 * While the summary endpoint is served from a transactional store and the results are strongly
 * consistent, the aggregate endpoint is served from our columnar store and the results are
 * only eventually consistent. On the other hand, the aggregate endpoint uses a
 * store that is more appropriate for analytics, makes use of approximation algorithms
 * (e.g for cardinality), and is generally faster and can do more complex aggregations.
 *
 * For usage, see the [Aggregation documentation](https://xata.io/docs/sdk/aggregate).
 */
export const aggregateTable = (variables: AggregateTableVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.AggResponse,
    AggregateTableError,
    AggregateTableRequestBody,
    {},
    {},
    AggregateTablePathParams
  >({
    url: '/db/{dbBranchName}/tables/{tableName}/aggregate',
    method: 'post',
    ...variables,
    signal
  });

export type FileAccessPathParams = {
  /**
   * The File Access Identifier
   */
  fileId: Schemas.FileAccessID;
  workspace: string;
  region: string;
};

export type FileAccessQueryParams = {
  /**
   * File access signature
   */
  verify?: Schemas.FileSignature;
};

export type FileAccessError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type FileAccessVariables = {
  pathParams: FileAccessPathParams;
  queryParams?: FileAccessQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * Retrieve file content by access id
 */
export const fileAccess = (variables: FileAccessVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Blob, FileAccessError, undefined, {}, FileAccessQueryParams, FileAccessPathParams>({
    url: '/file/{fileId}',
    method: 'get',
    ...variables,
    signal
  });

export type FileUploadPathParams = {
  /**
   * The File Access Identifier
   */
  fileId: Schemas.FileAccessID;
  workspace: string;
  region: string;
};

export type FileUploadQueryParams = {
  /**
   * File access signature
   */
  verify?: Schemas.FileSignature;
};

export type FileUploadError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
>;

export type FileUploadVariables = {
  body?: Blob;
  pathParams: FileUploadPathParams;
  queryParams?: FileUploadQueryParams;
} & DataPlaneFetcherExtraProps;

/**
 * Upload file using an upload url
 */
export const fileUpload = (variables: FileUploadVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.PutFileResponse, FileUploadError, Blob, {}, FileUploadQueryParams, FileUploadPathParams>({
    url: '/file/{fileId}',
    method: 'put',
    ...variables,
    signal
  });

export type SqlQueryPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type SqlQueryError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 503;
      payload: Responses.ServiceUnavailableError;
    }
>;

export type SqlQueryRequestBody = Schemas.PreparedStatement & {
  consistency?: Schemas.SQLConsistency;
  responseType?: Schemas.SQLResponseType;
};

export type SqlQueryVariables = {
  body: SqlQueryRequestBody;
  pathParams: SqlQueryPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Run an SQL query across the database branch.
 */
export const sqlQuery = (variables: SqlQueryVariables, signal?: AbortSignal) =>
  dataPlaneFetch<Responses.SQLResponse, SqlQueryError, SqlQueryRequestBody, {}, {}, SqlQueryPathParams>({
    url: '/db/{dbBranchName}/sql',
    method: 'post',
    ...variables,
    signal
  });

export type SqlBatchQueryPathParams = {
  /**
   * The DBBranchName matches the pattern `{db_name}:{branch_name}`.
   */
  dbBranchName: Schemas.DBBranchName;
  workspace: string;
  region: string;
};

export type SqlBatchQueryError = Fetcher.ErrorWrapper<
  | {
      status: 400;
      payload: Responses.BadRequestError;
    }
  | {
      status: 401;
      payload: Responses.AuthError;
    }
  | {
      status: 404;
      payload: Responses.SimpleError;
    }
  | {
      status: 503;
      payload: Responses.ServiceUnavailableError;
    }
>;

export type SqlBatchQueryRequestBody = {
  /**
   * The SQL statements.
   *
   * @x-go-type []sqlproxy.PreparedStatement
   */
  statements: Schemas.PreparedStatement[];
  consistency?: Schemas.SQLConsistency;
  responseType?: Schemas.SQLResponseType;
};

export type SqlBatchQueryVariables = {
  body: SqlBatchQueryRequestBody;
  pathParams: SqlBatchQueryPathParams;
} & DataPlaneFetcherExtraProps;

/**
 * Run multiple SQL queries across the database branch.
 */
export const sqlBatchQuery = (variables: SqlBatchQueryVariables, signal?: AbortSignal) =>
  dataPlaneFetch<
    Responses.SQLBatchResponse,
    SqlBatchQueryError,
    SqlBatchQueryRequestBody,
    {},
    {},
    SqlBatchQueryPathParams
  >({
    url: '/db/{dbBranchName}/sql/batch',
    method: 'post',
    ...variables,
    signal
  });

export const operationsByTag = {
  cluster: { listClusterBranches, getClusterMetrics },
  migrations: {
    applyMigration,
    startMigration,
    completeMigration,
    rollbackMigration,
    adaptTable,
    adaptAllTables,
    getBranchMigrationJobStatus,
    getMigrationJobs,
    getMigrationJobStatus,
    getMigrationHistory,
    getSchema,
    getSchemas,
    getBranchMigrationHistory,
    getBranchMigrationPlan,
    executeBranchMigrationPlan,
    getBranchSchemaHistory,
    compareBranchWithUserSchema,
    compareBranchSchemas,
    updateBranchSchema,
    previewBranchSchemaEdit,
    applyBranchSchemaEdit,
    pushBranchMigrations
  },
  branch: {
    getBranchList,
    getBranchDetails,
    createBranch,
    deleteBranch,
    copyBranch,
    getBranchMoveStatus,
    moveBranch,
    updateBranchMetadata,
    getBranchMetadata,
    getBranchStats,
    getGitBranchesMapping,
    addGitBranchesEntry,
    removeGitBranchesEntry,
    resolveBranch
  },
  database: { getDatabaseSettings, updateDatabaseSettings },
  migrationRequests: {
    queryMigrationRequests,
    createMigrationRequest,
    getMigrationRequest,
    updateMigrationRequest,
    listMigrationRequestsCommits,
    compareMigrationRequest,
    getMigrationRequestIsMerged,
    mergeMigrationRequest
  },
  table: {
    createTable,
    deleteTable,
    updateTable,
    getTableSchema,
    setTableSchema,
    getTableColumns,
    addTableColumn,
    getColumn,
    updateColumn,
    deleteColumn
  },
  records: {
    branchTransaction,
    insertRecord,
    getRecord,
    insertRecordWithID,
    updateRecordWithID,
    upsertRecordWithID,
    deleteRecord,
    bulkInsertTableRecords
  },
  files: {
    getFileItem,
    putFileItem,
    deleteFileItem,
    getFile,
    putFile,
    deleteFile,
    fileAccess,
    fileUpload
  },
  searchAndFilter: {
    queryTable,
    searchBranch,
    searchTable,
    vectorSearchTable,
    askTable,
    askTableSession,
    summarizeTable,
    aggregateTable
  },
  sql: { sqlQuery, sqlBatchQuery }
};
